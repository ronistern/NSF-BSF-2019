We propose to study two ways in which observations -- i.e., trajectories of actions performed by the agent in the past -- can help to meet this complexity challenge. The first is to use the given observations to automatically evaluate how different planning algorithms and heuristics perform, and adapt the planning algorithms accordingly. The second is to learn from the given observations special cases of the general planning problem that can be solved efficiently in the domain at hand. We detail both  approaches below. 


% Option #2: Find tractable classes
\subsubsection{Learning Tractable Classes for Planning}
% Classical planning is hard, but we can solve many problems. How to know which one?
While even classical planning is PSPACE complete~\cite{bylander1994computational}, 
current state-of-the-art planners are able to solve relatively large problems with hundreds of state variables. This success is often attributed to effective heuristic functions and smart planning algorithms that use them. However, it is often the case that one does not know upfront whether a given planning problem will be easy to solve or hard using current planners. It is known that some classes of the planning problem can be solved optimally in polynomial time~\cite{katz2008new}. In addition, the complexity of planning is known to be related to various characteristics of the underlying planning problem, such as the size of the causal graph~\cite{gimenez2012influence}, its relation to the delete relaxation problem~\cite{hoffmann2011analyzing}, and the problem's width~\cite{lipovetzky2012width}. All these works analyze the underlying structure of a planning domain to recognize structures that the respective planning algorithm can solve effectively. %identify if it can be solved easily. %However, a general 

% A key research question - can we learn from experience the tractable problems we can handle, and can we use experience to extend the range of tractable problems
We propose to identify such tractable, or probably tractable, structures in the planning world in an automated way. This is based on the work of PI Juba on learning probably applicable 
actions in a Partially Observable MDP setting~\cite{juba2016jmlr}. He proposed 
an algorithm for learning from observed trajectories sequences of actions that are probably applicable and then, when applicable, use these sequence of actions to achieve planning goals. 
In such cases, planning is almost instantaneously, and the probability of its success is bounded~\cite{juba2016jmlr}. 


% Brief intro to macro actions
Transferring this idea to classical planning allows a principled way to learn {\em macro actions}. A macro action is a sequence of actions the agent can perform that are considered as a single action when planning. Prior work has studies offline and online methods to learn sequence of actions that will be used in planning as macro action to speed up the search~\cite{chrpa2014mum,coles2007marvin,chrpa2015online,koedinger1990abstract,korf1985macro}. A key challenge is how to choose which macro actions to define, since adding macro actions to the set of actions an agent considers when planning can be useful, as it allows reaching deeper states in the search space faster, but it can also be harmful, as adding them increases the branching factor and creates more transpositions. We will develop algorithms for appropriately choosing which sequence of actions should be macros that are based on the frequency of the observed trajectories and the expected usefulness.  


% We can generate all macros and their exact pre and post conditions
There are well-known algorithms to identify all sequences of actions that occur sufficiently frequently in a collection of trajectories~\cite{mannila1997sequences}. The number of such candidate macro actions can be easily bounded by a polynomial in the frequency and the time horizon of the episodes. Because classical planning domains provide complete descriptions of the preconditions and effects of actions, we can easily generate a combined precondition and list of effects for each such candidate macro action.

% The question: how to choose effective macros
The theoretical and practical question that arises is which macro actions to construct that will be most useful to the planner. 
So, considering the observed trajectories, we can choose macro actions that would have been frequently useful in hindsight. 

% The problem: the macro-enabled planner is modified, and who knows where it will end up in
However, the theoretical challenge that now arises is whether or not the macro actions we construct remain useful to the problem in future problems: Observe that even if the trajectories were generated by a fixed planning algorithm on problems drawn from a common distribution, and the planner continues to operate on problems drawn from the same distribution, the availability of new macro actions may cause the planner to include a macro action that is distinct from the action it would have used at a given point in a given plan. Then the trajectories generated by the planner may, for example, include states that would never have been visited by the planner that did not have the macro actions available, and the rest of the macro actions may no longer be useful in this new region of the state space. What we do know is that the macro actions correspond to sequences of actions that occurred frequently in our past trajectories. Intuitively, as long as the states in which the actions were taken were sufficiently ``typical,'' then we expect that the final state after executing the macro should also be relatively ``typical,'' and thus also expect that our estimates of the utility of our new macro actions should only be skewed by a limited amount, depending on just how typical the initial states were and how frequently the macros were used.


\note{Roni: The above is good. Mabye we can put some mathematical formalizm to better convince the reader. I will focus today on the MBD part below and then get back to this.}
%GENERALIZE THE MACRO
%DEFINE STATIONARITY 
%PROBABLY PLANS
%HOW TO CONCATENATE SEQUENCES AND STILL HAVE GUARANTEES
%SOME RAPID MIXING ASSUMPTINO (GETTING TOT HE SAME START STATE DISTRIBUTIONS)


% Option #2: Learn probable macros
%{\bf Learning Macros that will Probably Work.} 
%such macro actions,  which are added to the agent's set of actions when planning. 
%sequence of actions 
%to the set of actions an agent can perform Learning macro actions are known method for learning from trajectories how to improve planning~\cite{korf,others,chrpa2014mum}. 


